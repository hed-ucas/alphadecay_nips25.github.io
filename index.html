<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs">
  <meta name="keywords" content="AlphaDecay, Weight Decay, Large Language Models, LLM, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/hed-ucas">Di He</a><sup>1,2,3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=_5Ir0soAAAAJ&hl=en">Songjun Tu</a><sup>2,3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=I783HxYAAAAJ&view_op=list_works&sortby=pubdate">Ajay Jaiswal</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/site/mathshenli/home">Li Shen</a><sup>5</sup>,</span>
            <span class="author-block">
              <a href="https://yuangzh.github.io/">Ganzhao Yuan</a><sup>6</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=73IbXtsAAAAJ&hl=en">Shiwei Liu</a><sup>7</sup>,</span>
            <span class="author-block">
              <a href="https://luuyin.com/">Lu Yin</a><sup>8</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,</span>
            <span class="author-block"><sup>2</sup>Peng Cheng Laboratory,</span>
            <span class="author-block"><sup>3</sup>University of Chinese Academy of Sciences,</span>
            <span class="author-block"><sup>4</sup>University of Texas at Austin,</span>
            <span class="author-block"><sup>5</sup>Shenzhen Campus of Sun Yat-sen University,</span>
            <span class="author-block"><sup>6</sup>Shenzhen University of Advanced Technology， </span>
            <span class="author-block"><sup>7</sup>University of Oxford， </span>
            <span class="author-block"><sup>8</sup>University of Surrey， </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://export.arxiv.org/abs/2506.14562"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://export.arxiv.org/abs/2506.14562"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/hed-ucas/alphadecay_nips25.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./images/AlphaDecay.png" alt="AlphaDecay Overview" style="width: 100%; max-width: 800px; margin: 0 auto; display: block;">
      <h2 class="subtitle has-text-centered">
        AlphaDecay determines the weight decay parameter values of each module in LLM training through spectral characteristics of the ESD distribution.
      </h2>
    </div>
  </div>
</section>

<!-- Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
        <div class="content has-text-justified">
          <p>
            We propose <strong>AlphaDecay</strong>, an adaptive weight decay strategy that assigns module-wise decay strengths according to the spectral characteristics of each module in LLMs, leading to improved perplexity and generalization compared to uniform and existing adaptive decay methods.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules.
          </p>
          <p>
            In this paper, weintroduce <strong>AlphaDecay</strong>, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness."
          </p>
          <p>
            Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance.
          </p>
          <p>
            Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method Overview</h2>
    <div class="content has-text-centered">
      <img src="./images/method.png" alt="Method Comparison" style="width: 100%; max-width: 700px; margin: 20px auto; display: block;">
      <p class="has-text-justified" style="max-width: 800px; margin: 0 auto;">
        Comparison of AlphaDecay with baseline methods on GPT-nano/C4 and ViT-tiny/ImageNet-1K. AlphaDecay achieves the best performance across different architectures and datasets.
      </p>
    </div>
  </div>
</section>

<!-- Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experimental Results</h2>
    
    <!-- Main Results -->
    <div class="content">
      <h3 class="title is-4">Main Results: LLaMA Pre-training on C4 Dataset</h3>
      <p class="has-text-justified">
        Comparison with various weight decay scheduling strategies using Adam optimizer on pre-training various sizes of LLaMa models (60M, 135M, 350M, 1B) on C4 dataset. Validation perplexity (↓) is reported. All baselines are carefully tuned. AlphaDecay consistently outperforms uniform decay, AWD, and AdaDecay across all model sizes and weight decay values.
      </p>
      <div class="has-text-centered">
        <img src="./images/main_result.png" alt="Main Results" style="width: 100%; max-width: 900px; margin: 20px auto; display: block;">
      </div>
    </div>

    <!-- Module-wise Analysis -->
    <div class="content" style="margin-top: 50px;">
      <h3 class="title is-4">Module-wise Weight Decay and Spectral Analysis</h3>
      <p class="has-text-justified">
        Analysis of module-wise weight decay assignments and their correlation with spectral properties. The figure shows:
      </p>
      <ul class="has-text-justified">
        <li><strong>Top-left:</strong> PL_Alpha_Hill values for different modules, showing the heavy-tailedness of each layer</li>
        <li><strong>Top-right:</strong> Empirical Spectral Density (ESD) of weight matrices for different modules</li>
        <li><strong>Bottom:</strong> Dynamic weight decay assignments across layers and training iterations, demonstrating how AlphaDecay adaptively balances module-wise differences</li>
      </ul>
      <div class="has-text-centered">
        <img src="./images/more_stru.png" alt="Module-wise Analysis" style="width: 100%; max-width: 900px; margin: 20px auto; display: block;">
      </div>
    </div>

    <!-- Zero-shot Evaluation -->
    <div class="content" style="margin-top: 50px;">
      <h3 class="title is-4">Zero-shot Evaluation on Downstream Tasks</h3>
      <p class="has-text-justified">
        Zero-shot performance comparison on various downstream tasks including ARC-c, ARC-e, PIQA, Hellaswag, OBQA, Winogrande, and BOOLQ. AlphaDecay demonstrates superior generalization capability with the highest average score of 43.35, outperforming Uniform (41.50), AdaDecay (41.68), and AWD (41.41) methods.
      </p>
      <div class="has-text-centered">
        <img src="./images/zeros-shot.png" alt="Zero-shot Results" style="width: 100%; max-width: 900px; margin: 20px auto; display: block;">
      </div>
    </div>

  </div>
</section>

<!-- Installation and Usage -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Installation</h2>
    <div class="content">
      <h3 class="title is-4">Setup</h3>
      <p>Our repository is built on top of <a href="https://github.com/jiaweizzhao/GaLore">Galore</a> and <a href="https://github.com/jiaweizzhao/GaLore">ConvNeXt</a>. You can configure the environment using the following command lines:</p>
      <pre><code>conda create -n alphadecay python=3.9 -y
conda activate alphadecay
conda install -r requirements</code></pre>

      <h3 class="title is-4">Prepare Dataset</h3>
      <p>We utilized the publicly available C4 dataset and ImageNet-1K dataset, both of which can be accessed and downloaded from their respective official websites.</p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Usage</h2>
    <div class="content">
      <h3 class="title is-4">Pretraining LLama-130M on C4</h3>
      <pre><code>torchrun --nproc_per_node=2 --master_port=20301 --master_addr=localhost torchrun_main.py \
    --model_config configs/llama_130m.json \
    --optimizer adam \
    --seed 5 \
    --lr 0.001 \
    --batch_size 256 \
    --total_batch_size 512 \
    --num_training_steps 20000 \
    --warmup_steps 2000 \
    --weight_decay 0.00001 \
    --use_modulewise_wd \
    --alpha_positively_with_WD \
    --unbalancedWD_every 500 \
    --esd_metric_for_tb alpha \
    --assign_func tb_linear_map\
    --wd_min_ratio 0.6666 \
    --wd_max_ratio 3 \
    --dtype bfloat16 \
    --eval_every 1000 \
    --wandb_name ours-adam-1.5-3unb-130M-lr0.001-WD0.00001-repeat1 \
    --target_eval_tokens 10_000_000 \
    --save_every 10000</code></pre>

      <h3 class="title is-4">Pretraining ViT-tiny on Imagenet-1K</h3>
      <pre><code>torchrun --nproc_per_node=2 --master_port=20023 --master_addr=localhost main.py \
    --model vit_deit_tiny_patch16_224 \--drop_path 0.1 \
    --batch_size 192 \
    --lr 4e-3 \
    --weight_decay 0.000005 \
    --use_modulewise_wd \
    --alpha_positively_with_WD \
    --unbalancedWD_every 500 \
    --esd_metric_for_tb alpha \
    --assign_func tb_linear_map\
    --wd_min_ratio 0.6666 \
    --wd_max_ratio 3 \
    --update_freq 1 \
    --model_ema true \
    --model_ema_eval true \
    --data_path /home/hedi/ImageNet \
    --output_dir /home/hedi/LLM/ViT-awd/outputs \
    --wandb_name vit_deit_tiny_patch16_224-Ours-WD0.000005-1.5-5unb</code></pre>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{he2025alphadecay,
  title={AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs},
  author={He, Di and Jaiswal, Ajay and Tu, Songjun and Shen, Li and Yuan, Ganzhao and Liu, Shiwei and Yin, Lu},
  journal={arXiv preprint arXiv:2506.14562},
  year={2025}
}</code></pre>
  </div>
</section>

<!-- Acknowledgements -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    <p>
      This repository is built upon the <a href="https://github.com/jiaweizzhao/GaLore">Galore</a> and <a href="https://github.com/jiaweizzhao/GaLore">ConvNeXt</a> repositories. Thanks for their great work!
    </p>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://export.arxiv.org/abs/2506.14562">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/hed-ucas/alphadecay_nips25.github.io" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
