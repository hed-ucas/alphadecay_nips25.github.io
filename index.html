<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs">
  <meta name="keywords" content="AlphaDecay, Weight Decay, Large Language Models, LLM, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Di He</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Ajay Jaiswal</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="#">Songjun Tu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Li Shen</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="#">Ganzhao Yuan</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="#">Shiwei Liu</a><sup>5</sup>,</span>
            <span class="author-block">
              <a href="#">Lu Yin</a><sup>6</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Chinese Academy of Sciences,</span>
            <span class="author-block"><sup>2</sup>Georgia Institute of Technology,</span>
            <span class="author-block"><sup>3</sup>JD Explore Academy,</span>
            <span class="author-block"><sup>4</sup>Sun Yat-sen University,</span>
            <span class="author-block"><sup>5</sup>University of Oxford,</span>
            <span class="author-block"><sup>6</sup>Eindhoven University of Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://export.arxiv.org/abs/2506.14562"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://export.arxiv.org/abs/2506.14562"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/hed-ucas/alphadecay_nips25.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./images/AlphaDecay.png" alt="AlphaDecay Overview" style="width: 100%; max-width: 800px; margin: 0 auto; display: block;">
      <h2 class="subtitle has-text-centered">
        AlphaDecay determines the weight decay parameter values of each module in LLM training through spectral characteristics of the ESD distribution.
      </h2>
    </div>
  </div>
</section>

<!-- Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
        <div class="content has-text-justified">
          <p>
            We propose <strong>AlphaDecay</strong>, an adaptive weight decay strategy that assigns module-wise decay strengths according to the spectral characteristics of each module in LLMs, leading to improved perplexity and generalization compared to uniform and existing adaptive decay methods.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules.
          </p>
          <p>
            In this paper, we introduce <strong>AlphaDecay</strong>, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness."
          </p>
          <p>
            Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance.
          </p>
          <p>
            Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>
    
    <!-- Main Results -->
    <div class="content">
      <h3 class="title is-4">Main Results: Adam Optimizer on C4 Dataset</h3>
      <p class="has-text-justified">
        Comparison with various weight decay scheduling strategies using Adam optimizer on pre-training various sizes of LLaMa models on C4 dataset. Validation perplexity (↓) is reported. All baselines are carefully tuned.
      </p>
      <div class="table-container">
        <table class="table is-striped is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Weight Decay</th>
              <th colspan="3">LLaMa-60M</th>
              <th colspan="3">LLaMa-135M</th>
              <th colspan="3">LLaMa-350M</th>
              <th colspan="3">LLaMa-1B</th>
            </tr>
            <tr>
              <th></th>
              <th>1e-5</th>
              <th>5e-6</th>
              <th>1e-6</th>
              <th>1e-5</th>
              <th>5e-6</th>
              <th>1e-6</th>
              <th>1e-5</th>
              <th>5e-6</th>
              <th>1e-6</th>
              <th>1e-5</th>
              <th>5e-6</th>
              <th>1e-6</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>WD=0</td>
              <td>-</td>
              <td>33.23</td>
              <td>-</td>
              <td>-</td>
              <td>24.60</td>
              <td>-</td>
              <td>-</td>
              <td>18.62</td>
              <td>-</td>
              <td>-</td>
              <td>16.11</td>
              <td>-</td>
            </tr>
            <tr>
              <td>Uniform</td>
              <td>32.39</td>
              <td>32.56</td>
              <td>33.03</td>
              <td>22.99</td>
              <td>23.14</td>
              <td>24.14</td>
              <td>17.12</td>
              <td>16.74</td>
              <td>17.50</td>
              <td>15.36</td>
              <td>14.66</td>
              <td>15.03</td>
            </tr>
            <tr>
              <td>AWD</td>
              <td>33.78</td>
              <td>33.74</td>
              <td>33.74</td>
              <td>24.25</td>
              <td>24.45</td>
              <td>24.53</td>
              <td>18.32</td>
              <td>18.55</td>
              <td>18.79</td>
              <td>16.03</td>
              <td>16.22</td>
              <td>16.38</td>
            </tr>
            <tr>
              <td>Adadecay</td>
              <td>32.24</td>
              <td>32.52</td>
              <td>33.03</td>
              <td>23.20</td>
              <td>23.08</td>
              <td>23.96</td>
              <td>18.21</td>
              <td>17.42</td>
              <td>17.91</td>
              <td>17.23</td>
              <td>18.14</td>
              <td>15.35</td>
            </tr>
            <tr style="background-color: #fffacd;">
              <td><strong>AlphaDecay</strong></td>
              <td><strong>31.56</strong></td>
              <td><strong>31.58</strong></td>
              <td><strong>32.61</strong></td>
              <td><strong>22.76</strong></td>
              <td><strong>22.55</strong></td>
              <td><strong>23.49</strong></td>
              <td><strong>17.00</strong></td>
              <td><strong>16.66</strong></td>
              <td><strong>16.88</strong></td>
              <td><strong>15.13</strong></td>
              <td><strong>14.55</strong></td>
              <td><strong>14.63</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

    <!-- AdamW Results -->
    <div class="content">
      <h3 class="title is-4">AdamW Optimizer Results</h3>
      <p class="has-text-justified">
        Comparison of various weight decay scheduling strategies using AdamW optimizer for pre-training LLaMa-60M and LLaMa-130M models. Validation perplexity (↓) on the C4 dataset is reported.
      </p>
      <div class="table-container">
        <table class="table is-striped is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Weight Decay</th>
              <th colspan="3">LLaMa-60M</th>
              <th colspan="3">LLaMa-135M</th>
            </tr>
            <tr>
              <th></th>
              <th>0.1</th>
              <th>0.05</th>
              <th>0.01</th>
              <th>0.1</th>
              <th>0.05</th>
              <th>0.01</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>WD=0</td>
              <td>-</td>
              <td>32.73</td>
              <td>-</td>
              <td>-</td>
              <td>24.39</td>
              <td>-</td>
            </tr>
            <tr>
              <td>Uniform</td>
              <td>31.95</td>
              <td>32.31</td>
              <td>32.66</td>
              <td>23.32</td>
              <td>23.81</td>
              <td>24.28</td>
            </tr>
            <tr>
              <td>AWD</td>
              <td>32.58</td>
              <td>32.67</td>
              <td>32.67</td>
              <td>24.30</td>
              <td>24.35</td>
              <td>24.41</td>
            </tr>
            <tr>
              <td>Adadecay</td>
              <td>31.88</td>
              <td>32.25</td>
              <td>32.58</td>
              <td>23.18</td>
              <td>23.62</td>
              <td>24.21</td>
            </tr>
            <tr style="background-color: #fffacd;">
              <td><strong>AlphaDecay</strong></td>
              <td><strong>31.20</strong></td>
              <td><strong>31.65</strong></td>
              <td><strong>32.45</strong></td>
              <td><strong>22.66</strong></td>
              <td><strong>23.04</strong></td>
              <td><strong>23.98</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

    <!-- ViT Results -->
    <div class="content">
      <h3 class="title is-4">ViT-tiny on ImageNet-1K</h3>
      <p class="has-text-justified">
        Comparison of various weight decay scheduling strategies using the Adam optimizer for training ViT-tiny on ImageNet-1K. Top-1 accuracy (%) is reported on the ImageNet-1K validation set.
      </p>
      <div class="table-container">
        <table class="table is-striped is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Weight Decay</th>
              <th>Uniform</th>
              <th>AWD</th>
              <th>AdaDecay</th>
              <th>AlphaDecay</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>0</td>
              <td colspan="4">65.12</td>
            </tr>
            <tr>
              <td>5e-6</td>
              <td>66.97</td>
              <td>65.21</td>
              <td>65.86</td>
              <td><strong>67.40</strong></td>
            </tr>
            <tr>
              <td>1e-6</td>
              <td>66.41</td>
              <td>64.98</td>
              <td>66.26</td>
              <td><strong>67.73</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
  </div>
</section>

<!-- Installation and Usage -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Installation</h2>
    <div class="content">
      <h3 class="title is-4">Setup</h3>
      <p>Our repository is built on top of <a href="https://github.com/jiaweizzhao/GaLore">Galore</a> and <a href="https://github.com/jiaweizzhao/GaLore">ConvNeXt</a>. You can configure the environment using the following command lines:</p>
      <pre><code>conda create -n alphadecay python=3.9 -y
conda activate alphadecay
conda install -r requirements</code></pre>

      <h3 class="title is-4">Prepare Dataset</h3>
      <p>We utilized the publicly available C4 dataset and ImageNet-1K dataset, both of which can be accessed and downloaded from their respective official websites.</p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Usage</h2>
    <div class="content">
      <h3 class="title is-4">Pretraining LLama-130M on C4</h3>
      <pre><code>torchrun --nproc_per_node=2 --master_port=20301 --master_addr=localhost torchrun_main.py \
    --model_config configs/llama_130m.json \
    --optimizer adam \
    --seed 5 \
    --lr 0.001 \
    --batch_size 256 \
    --total_batch_size 512 \
    --num_training_steps 20000 \
    --warmup_steps 2000 \
    --weight_decay 0.00001 \
    --use_modulewise_wd \
    --alpha_positively_with_WD \
    --unbalancedWD_every 500 \
    --esd_metric_for_tb alpha \
    --assign_func tb_linear_map\
    --wd_min_ratio 0.6666 \
    --wd_max_ratio 3 \
    --dtype bfloat16 \
    --eval_every 1000 \
    --wandb_name ours-adam-1.5-3unb-130M-lr0.001-WD0.00001-repeat1 \
    --target_eval_tokens 10_000_000 \
    --save_every 10000</code></pre>

      <h3 class="title is-4">Pretraining ViT-tiny on Imagenet-1K</h3>
      <pre><code>torchrun --nproc_per_node=2 --master_port=20023 --master_addr=localhost main.py \
    --model vit_deit_tiny_patch16_224 \
    --drop_path 0.1 \
    --batch_size 192 \
    --lr 4e-3 \
    --weight_decay 0.000005 \
    --use_modulewise_wd \
    --alpha_positively_with_WD \
    --unbalancedWD_every 500 \
    --esd_metric_for_tb alpha \
    --assign_func tb_linear_map\
    --wd_min_ratio 0.6666 \
    --wd_max_ratio 3 \
    --update_freq 1 \
    --model_ema true \
    --model_ema_eval true \
    --data_path /home/hedi/ImageNet \
    --output_dir /home/hedi/LLM/ViT-awd/outputs \
    --wandb_name vit_deit_tiny_patch16_224-Ours-WD0.000005-1.5-5unb</code></pre>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{he2025alphadecay,
  title={AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs},
  author={He, Di and Jaiswal, Ajay and Tu, Songjun and Shen, Li and Yuan, Ganzhao and Liu, Shiwei and Yin, Lu},
  journal={arXiv preprint arXiv:2506.14562},
  year={2025}
}</code></pre>
  </div>
</section>

<!-- Acknowledgements -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    <p>
      This repository is built upon the <a href="https://github.com/jiaweizzhao/GaLore">Galore</a> and <a href="https://github.com/jiaweizzhao/GaLore>ConvNeXt</a> repositories. Thanks for their great work!
    </p>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://export.arxiv.org/abs/2506.14562">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/hed-ucas/alphadecay_nips25.github.io" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
